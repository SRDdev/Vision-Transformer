{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer\n",
    "\n",
    "In this notebook we will be replicating the [\"An Image is Worth 16x16 Words: Transformers for Image Recognition\"](https://arxiv.org/abs/2010.11929) Vision Transformer from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops \n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import Compose,Resize,ToTensor,Normalize,RandomHorizontalFlip,RandomCrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'device : {device}')\n",
    "#------------------------Hyperparams--------------#\n",
    "patch_size = 16\n",
    "latent_size = 768 \n",
    "n_channels = 3\n",
    "num_heads = 12\n",
    "num_encoders = 12\n",
    "dropout = 0.1\n",
    "num_classes = 10\n",
    "size = 224\n",
    "\n",
    "epochs = 10\n",
    "base_lr = 10e-3\n",
    "weight_decay = 0.03\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------Linear Inupt Projection---------------------------------#\n",
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self,patch_size=patch_size,n_channels=n_channels,device=device,latent_size=latent_size,batch_size=batch_size):\n",
    "        \"\"\"Initialize all the variables\"\"\"\n",
    "\n",
    "        super(InputEmbedding,self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.patch_size = patch_size\n",
    "        self.n_channels = n_channels\n",
    "        self.input_size = self.patch_size*self.patch_size*self.n_channels   # width*height*channels\n",
    "        \n",
    "        # Linear Projection\n",
    "        self.LinearProjection = nn.Linear(self.input_size,self.latent_size)\n",
    "\n",
    "        # Class Token\n",
    "        self.class_token = nn.Parameter(torch.rand(self.batch_size,1,self.latent_size)).to(self.device)\n",
    "\n",
    "        # Positional Embedding\n",
    "        self.pos_embedding = nn.Parameter(torch.rand(self.batch_size,1,self.latent_size)).to(self.device)\n",
    "\n",
    "    def forward(self,input_data):\n",
    "        \"\"\"\n",
    "        Takes the input data in batch_size and creates patches. Applys Linear Projection on it and adds class_token and pos_embedding to each patch.\n",
    "\n",
    "        Args : \n",
    "            Input : input_data.\n",
    "            Output: input_patches and pos_embedding.\n",
    "        \"\"\"\n",
    "        input_data = input_data.to(self.device)\n",
    "        patches = einops.rearrange(\n",
    "            input_data, 'b c (h h1) (w w1) -> b (h w) (h1 w1 c)',\n",
    "            h1 = self.patch_size , w1 = self.patch_size\n",
    "            )\n",
    "        \n",
    "        print(f'Input data : {input_data.shape}')\n",
    "        print(f'Patches : {patches.shape}')\n",
    "\n",
    "        linear_projection = self.LinearProjection(patches).to(self.device)\n",
    "        b,n,_ = linear_projection.shape \n",
    "        linear_projection = torch.cat((self.class_token,linear_projection),dim=1)\n",
    "        pos_embed = einops.repeat(self.pos_embedding, ' b 1 d -> b m d' , m = n+1)\n",
    "        print(f'Linear Projection : {linear_projection.size()}')\n",
    "        print(f'Positional : {pos_embed.size()}')\n",
    "        \n",
    "        linear_projection = linear_projection+pos_embed\n",
    "\n",
    "        return linear_projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data : torch.Size([1, 3, 224, 224])\n",
      "Patches : torch.Size([1, 196, 768])\n",
      "Linear Projection : torch.Size([1, 197, 768])\n",
      "Positional : torch.Size([1, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "# Understanding Code\n",
    "test_input = torch.randn((1,3,224,224))\n",
    "test_class = InputEmbedding().to(device)\n",
    "embed_test = test_class(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------Encoder----------------------#\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,latent_size=latent_size,num_heads=num_heads,device=device,dropout= dropout):\n",
    "        \"\"\"\n",
    "        Initialize variables for Encoder Layer.\n",
    "        \"\"\"\n",
    "        super(EncoderBlock,self).__init__()\n",
    "\n",
    "        self.latent_size = latent_size\n",
    "        self.num_heads = num_heads\n",
    "        self.device = device\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Normalization \n",
    "        self.normalize = nn.LayerNorm(self.latent_size) \n",
    "        self.multihhead = nn.MultiheadAttention(self.latent_size,self.num_heads,dropout=self.dropout)\n",
    "        self.enc_MLP = nn.Sequential(\n",
    "            nn.Linear(self.latent_size,self.latent_size*4),          # Given in the paper (output of linear is 4 times)\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.latent_size*4,self.latent_size),          # Given in the paper (output is back to latent_size)  \n",
    "            nn.Dropout(self.dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self,embedded_patches):\n",
    "        \"\"\"\n",
    "        Define : \n",
    "        Args: \n",
    "            Input : \n",
    "            Output : \n",
    "        \"\"\"\n",
    "        firstnorm_out = self.normalize(embedded_patches)\n",
    "        attention_output = self.multihhead(firstnorm_out,firstnorm_out,firstnorm_out)[0]          # return a tuple from which we take only the 1st value.\n",
    "        first_resudial = attention_output + embedded_patches\n",
    "\n",
    "        secondnorm_out = self.normalize(first_resudial)\n",
    "        mlp_output = self.enc_MLP(secondnorm_out)\n",
    "        \n",
    "\n",
    "        return mlp_output + first_resudial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.4165,  1.2501,  0.9870,  ...,  1.0960,  0.9106,  0.8004],\n",
       "         [ 0.0430,  1.0837, -0.2006,  ...,  1.2386,  1.0093,  2.0220],\n",
       "         [ 0.7353,  0.8299,  1.2798,  ..., -0.0305,  1.8974, -0.5246],\n",
       "         ...,\n",
       "         [ 0.1577,  0.4087,  0.3084,  ...,  0.5530,  1.5896,  0.0213],\n",
       "         [ 0.3268,  0.6979, -0.2313,  ...,  2.1076,  0.3836, -0.2035],\n",
       "         [-0.3212, -0.8197, -0.2331,  ...,  0.7070, -0.3289,  1.0611]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_encoder = EncoderBlock().to(device)\n",
    "test_encoder(embed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------ViT--------------------------#\n",
    "class VisionTransfomer(nn.Module):\n",
    "    def __init__(self, num_encoders=num_encoders, latent_size=latent_size, device=device, num_classes=num_classes, dropout=dropout):\n",
    "        super(VisionTransfomer, self).__init__()\n",
    "        self.num_encoder = num_encoders\n",
    "        self.latent_size = latent_size\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = InputEmbedding()\n",
    "\n",
    "        # Create the stack of encoders\n",
    "        self.encStack = nn.ModuleList([EncoderBlock() for i in range(self.num_encoder)])\n",
    "\n",
    "        self.MLP_head = nn.Sequential(\n",
    "            nn.LayerNorm(self.latent_size),\n",
    "            nn.Linear(self.latent_size, self.latent_size),\n",
    "            nn.Linear(self.latent_size, self.num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, test_input):\n",
    "        enc_output = self.embedding(test_input)\n",
    "\n",
    "        for enc_layer in self.encStack:\n",
    "            enc_output = enc_layer(enc_output)\n",
    "\n",
    "        cls_token_embed = enc_output[:, 0]\n",
    "\n",
    "        return self.MLP_head(cls_token_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data : torch.Size([1, 3, 224, 224])\n",
      "Patches : torch.Size([1, 196, 768])\n",
      "Linear Projection : torch.Size([1, 197, 768])\n",
      "Positional : torch.Size([1, 197, 768])\n",
      "tensor([[-0.0344,  0.0879, -0.1353,  0.2790, -0.4280,  0.4329,  0.2900,  0.1555,\n",
      "         -0.3983, -0.0051]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "model = VisionTransfomer().to(device)\n",
    "vit_output = model(test_input)\n",
    "print(vit_output)\n",
    "print(vit_output.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
